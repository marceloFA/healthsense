{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from joblib import dump, load\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# old classification metrics\n",
    "from sklearn.metrics import ( classification_report, confusion_matrix, \n",
    "                             precision_score, accuracy_score, \n",
    "                             precision_recall_fscore_support )\n",
    "# new classification metrics\n",
    "from sklearn.metrics import make_scorer, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(confusion_matrix_file_name):\n",
    "    '''\n",
    "        read a confusion matrix from a file, and calculate some metrics\n",
    "        input: a pd.DataFrame representing a confusion matrix\n",
    "        output: a pd.DataFrame containing the metrics\n",
    "    '''\n",
    "    confusion_matrix = pd.read_csv(confusion_matrix_file_name)\n",
    "    \n",
    "    if 'Unnamed: 0' in confusion_matrix.columns:\n",
    "        confusion_matrix.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    \n",
    "    confusion_matrix = confusion_matrix.values\n",
    "    metrics = {}\n",
    "    \n",
    "    FP = metrics['False Positive'] = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)  \n",
    "    FN = metrics['False Negative'] = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = metrics['True Positive'] = np.diag(confusion_matrix)\n",
    "    TN = metrics['True Negative'] = confusion_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    metrics['True Positive Rate'] = TP/(TP+FN)\n",
    "    # Specificity or true negative rate\n",
    "    metrics['True Negative Rate'] = TN/(TN+FP) \n",
    "    # Precision or positive predictive value\n",
    "    metrics['Precision'] = TP/(TP+FP)\n",
    "    # Fall out or false positive rate ##########\n",
    "    metrics['False Positive Rate'] = FP/(FP+TN)\n",
    "    # False negative rate ############\n",
    "    metrics['False Negative Rate'] = FN/(TP+FN)\n",
    "    # Overall accuracy\n",
    "    metrics['Overall Accuracy'] = (TP+TN)/(TP+FP+FN+TN)\n",
    "    \n",
    "    #for metric, values in metrics.items():\n",
    "    #print(f'{metric} has {len(values)} lines\\n')\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_step_1 = get_metrics('confusion_matrix_1.csv')\n",
    "metrics_step_2 = get_metrics('confusion_matrix_2.csv')\n",
    "metrics_step_3 = get_metrics('confusion_matrix_3.csv')\n",
    "metrics_step_4 = get_metrics('confusion_matrix_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot metrics here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_auc_score(y_test, probabilities):\n",
    "    '''\n",
    "        Calculate 4 different averages for the roc auc score\n",
    "    '''\n",
    "    # https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#area-under-roc-for-the-multiclass-problem\n",
    "    \n",
    "    roc_auc = {}\n",
    "    roc_auc['OVR Macro Average'] = roc_auc_score(y_test, probabilities, \n",
    "                                      multi_class=\"ovr\", average=\"macro\")\n",
    "    roc_auc['OVR Weighted Average'] = roc_auc_score(y_test, probabilities, \n",
    "                                         multi_class=\"ovr\", average=\"weighted\")\n",
    "    roc_auc['OVR Micro Average'] = roc_auc_score(y_test, probabilities, \n",
    "                                      multi_class=\"ovr\", average=\"micro\")\n",
    "    roc_auc['OVR Samples Average'] = roc_auc_score(y_test, probabilities, \n",
    "                                         multi_class=\"ovr\", average=\"samples\")\n",
    "\n",
    "    return roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_roc_auc_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8c46266d08f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save roc auc scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mroc_auc_step_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_roc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_step_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities_step_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mroc_auc_step_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_roc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_step_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities_step_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mroc_auc_step_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_roc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_step_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities_step_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mroc_auc_step_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_roc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_step_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities_step_4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_roc_auc_score' is not defined"
     ]
    }
   ],
   "source": [
    "# save roc auc scores\n",
    "roc_auc_step_1 = get_roc_auc_score(y_test_step_1, probabilities_step_1)\n",
    "roc_auc_step_2 = get_roc_auc_score(y_test_step_2, probabilities_step_2)\n",
    "roc_auc_step_3 = get_roc_auc_score(y_test_step_3, probabilities_step_3)\n",
    "roc_auc_step_4 = get_roc_auc_score(y_test_step_4, probabilities_step_4)\n",
    "\n",
    "# make a dataframe containing roc_auc_scores from all 4 steps\n",
    "index = ['Step 1', 'Step 2', 'Step 3', 'DETECT']\n",
    "roc_auc_scores_df = pd.Dataframe([roc_auc_step_1, roc_auc_step_2, roc_auc_step_3, roc_auc_step_4], index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot roc_auc_score here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
